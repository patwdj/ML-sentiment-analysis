{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "disabled-retro",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tfidf_train = pd.read_csv(\"train_tfidf.csv\", header = 0, converters={'tweet': lambda x: x[2:-2].split('), (')})\n",
    "tfidf_dev = pd.read_csv(\"dev_tfidf.csv\", header = 0, converters={'tweet': lambda x: x[2:-2].split('), (')})\n",
    "tfidf_test = pd.read_csv(\"test_tfidf.csv\", header = 0, converters={'tweet': lambda x: x[2:-2].split('), (')})\n",
    "\n",
    "word_count_train = pd.read_csv(\"train_count.csv\", header = 0, converters={'tweet': lambda x: x[2:-2].split('), (')})\n",
    "word_count_dev = pd.read_csv(\"dev_count.csv\", header = 0, converters={'tweet': lambda x: x[2:-2].split('), (')})\n",
    "word_count_test = pd.read_csv(\"test_count.csv\", header = 0, converters={'tweet': lambda x: x[2:-2].split('), (')})\n",
    "\n",
    "glove_train = pd.read_csv(\"train_glove.csv\", header = 0, converters={'tweet': lambda x: x[1:-1].split(',')})\n",
    "glove_dev = pd.read_csv(\"dev_glove.csv\", header = 0, converters={'tweet': lambda x: x[1:-1].split(',')})\n",
    "glove_test = pd.read_csv(\"test_glove.csv\", header = 0, converters={'tweet': lambda x: x[1:-1].split(',')})\n",
    "\n",
    "\n",
    "#rename columns for clarity\n",
    "word_count_train.rename(columns={'tweet': 'word_count'}, inplace=True)\n",
    "word_count_dev.rename(columns={'tweet': 'word_count'}, inplace=True)\n",
    "word_count_test.rename(columns={'tweet': 'word_count'}, inplace=True)\n",
    "\n",
    "glove_train.rename(columns={'tweet': 'glove'}, inplace=True)\n",
    "glove_dev.rename(columns={'tweet': 'glove'}, inplace=True)\n",
    "glove_test.rename(columns={'tweet': 'glove'}, inplace=True)\n",
    "\n",
    "tfidf_train.rename(columns={'tweet': 'tfidf'}, inplace=True)\n",
    "tfidf_dev.rename(columns={'tweet': 'tfidf'}, inplace=True)\n",
    "tfidf_test.rename(columns={'tweet': 'tfidf'}, inplace=True)\n",
    "\n",
    "y_train = word_count_train['sentiment']\n",
    "y_dev = word_count_dev['sentiment']\n",
    "y_test = word_count_test['sentiment']\n",
    "\n",
    "test_id = word_count_test['tweet_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-george",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = open('vocab.txt', 'r')\n",
    "vocab_lines = vocab_file.readlines()\n",
    "vocab_set = []\n",
    "vocab_list = []\n",
    "\n",
    "for line in vocab_lines:\n",
    "    line = line.strip()\n",
    "    line = line.split('\\t')\n",
    "    word = line[0]\n",
    "    word_id = int(line[1])\n",
    "    vocab_set.append((word_id,word))\n",
    "    \n",
    "vocab_set.sort()\n",
    "\n",
    "for (index,word) in vocab_set:\n",
    "    vocab_list.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-genome",
   "metadata": {},
   "source": [
    "# Prepare Data (To Array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toTupleArray(data, type):\n",
    "    data_clean = []\n",
    "    for instance in data:\n",
    "        instance_new = []\n",
    "        for word in instance:\n",
    "            word = word.strip()\n",
    "            word = word.split(\",\")\n",
    "            instance_new.append(((int(word[0])), type(word[1])))\n",
    "        data_clean.append(instance_new)\n",
    "    return data_clean\n",
    "\n",
    "def toArray(data):\n",
    "    data_clean = []\n",
    "    for instance in data:\n",
    "        instance_new =[]\n",
    "        for index in instance:\n",
    "            instance_new.append(float(index))\n",
    "        data_clean.append(instance_new)    \n",
    "    return data_clean\n",
    "\n",
    "\n",
    "w_train = word_count_train['word_count']\n",
    "w_dev = word_count_dev['word_count']\n",
    "w_test = word_count_test['word_count']\n",
    "\n",
    "t_train = tfidf_train['tfidf']\n",
    "t_dev = tfidf_dev['tfidf']\n",
    "t_test = tfidf_test['tfidf']\n",
    "\n",
    "g_train = glove_train['glove']\n",
    "g_dev = glove_dev['glove']\n",
    "g_test = glove_test['glove']\n",
    "\n",
    "w_train = toTupleArray(w_train, int)\n",
    "w_dev = toTupleArray(w_dev, int)\n",
    "w_test = toTupleArray(w_test, int)\n",
    "\n",
    "t_train = toTupleArray(t_train, float)\n",
    "t_dev = toTupleArray(t_dev, float)\n",
    "t_test = toTupleArray(t_test, float)\n",
    "\n",
    "g_train = toArray(g_train)\n",
    "g_dev = toArray(g_dev)\n",
    "g_test = toArray(g_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-clock",
   "metadata": {},
   "source": [
    "# Vectorize Data (for TF-IDF and Word Count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-ecuador",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize5000(array):\n",
    "    matrix = [[0 for i in range(5000)] for i in range(len(array))]\n",
    "    for instance_index in range(len(array)):\n",
    "        for word in array[instance_index]:\n",
    "            matrix[instance_index][word[0]] = word[1]\n",
    "    return matrix\n",
    "        \n",
    "t_train = vectorize5000(t_train)\n",
    "t_dev = vectorize5000(t_dev)\n",
    "t_test = vectorize5000(t_test)\n",
    "\n",
    "w_train = vectorize5000(w_train)\n",
    "w_dev = vectorize5000(w_dev)\n",
    "w_test = vectorize5000(w_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-chest",
   "metadata": {},
   "source": [
    "# Normalize Data (for Glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-zoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_instances = [ [] for _ in range(len(g_train[0]))]\n",
    "    \n",
    "for instance in g_train:\n",
    "    for feature_index in range(len(g_train[0])):\n",
    "        features_instances[feature_index].append(instance[feature_index])\n",
    "\n",
    "#find mean and standard deviation of train data\n",
    "mean = []\n",
    "stdev = []\n",
    "for feature in features_instances:\n",
    "    mean.append(sum(feature)/len(feature))\n",
    "    stdev.append(np.std(feature))\n",
    "\n",
    "g_full = g_train + g_dev + g_test\n",
    "\n",
    "g_norm = []\n",
    "\n",
    "#standardise using the mean and standard deviation of train data\n",
    "for instance in g_full:\n",
    "    norm_instance = []\n",
    "    for feature_index in range(len(g_full[0])):\n",
    "        norm_instance.append((instance[feature_index]-mean[feature_index])/stdev[feature_index])\n",
    "    g_norm.append(norm_instance)\n",
    "\n",
    "g_train = g_norm[0:len(g_train)]\n",
    "g_dev = g_norm[len(g_train):(len(g_train)+len(g_dev))]\n",
    "g_test = g_norm[(len(g_train)+len(g_dev))::]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-springfield",
   "metadata": {},
   "source": [
    "# Feature Sets    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-virgin",
   "metadata": {},
   "source": [
    "As there are only 3 features, the wrapper method will be used for feature selection. There are 7 possible combination of features: word_count only (w), tfidf only (t), glove only (g), word_count & tfidf (wt), word_count & glove (wg), tfidf & glove (tg), word_count, tfidf, & glove (wtg).\n",
    "\n",
    "As word_count and tfidf are likely to be correlated, combinations with them together are excluded. \n",
    "Thus the following are the feature sets that will be tested\n",
    "1. word_count only (w)\n",
    "2. tfidf only (t)\n",
    "3. glove only (g)\n",
    "4. word_count & glove (wg)\n",
    "5. tfidf & glove (tg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-amplifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(w_train) == len(t_train) == len(g_train))\n",
    "assert(len(w_dev) == len(t_dev) == len(g_dev))\n",
    "assert(len(w_test) == len(t_test) == len(g_test))\n",
    "\n",
    "wg_train = []\n",
    "wg_dev = []\n",
    "wg_test = []\n",
    "\n",
    "tg_train = []\n",
    "tg_dev = []\n",
    "tg_test = []\n",
    "\n",
    "for instance_index in range(len(w_train)):\n",
    "    instance_wg = w_train[instance_index] + g_train[instance_index]\n",
    "    instance_tg = t_train[instance_index] + g_train[instance_index]\n",
    "    \n",
    "    wg_train.append(instance_wg)\n",
    "    tg_train.append(instance_tg)\n",
    "\n",
    "for instance_dev in range(len(w_dev)):  \n",
    "    instance_wg = w_dev[instance_dev] + g_dev[instance_dev]\n",
    "    instance_tg = t_dev[instance_dev] + g_dev[instance_dev]\n",
    "\n",
    "    wg_dev.append(instance_wg)\n",
    "    tg_dev.append(instance_tg)\n",
    "    \n",
    "for instance_test in range(len(w_test)):\n",
    "    instance_wg = w_test[instance_test] + g_test[instance_test]\n",
    "    instance_tg = t_test[instance_test] + g_test[instance_test]\n",
    "    \n",
    "    wg_test.append(instance_wg)\n",
    "    tg_test.append(instance_tg)\n",
    "\n",
    "\n",
    "print(\"word_count only train shape: \", (len(w_train), len(w_train[0])))\n",
    "print(\"word_count only dev shape: \", (len(w_dev), len(w_dev[0])))\n",
    "print(\"word_count only test shape: \", (len(w_test), len(w_test[0])))\n",
    "\n",
    "print(\"tfidf only train shape: \", (len(t_train), len(t_train[0])))\n",
    "print(\"tfidf only dev shape: \", (len(t_dev), len(t_dev[0])))\n",
    "print(\"tfidf only test shape: \", (len(t_test), len(t_test[0])))\n",
    "\n",
    "print(\"glove only train shape: \", (len(g_train), len(g_train[0])))\n",
    "print(\"glove only dev shape: \", (len(g_dev), len(g_dev[0])))\n",
    "print(\"glove only test shape: \", (len(g_test), len(g_test[0])))\n",
    "\n",
    "print(\"word_count and glove train shape: \", (len(wg_train), len(wg_train[0])))\n",
    "print(\"word_count and glove dev shape: \", (len(wg_dev), len(wg_dev[0])))\n",
    "print(\"word_count and glove test shape: \", (len(wg_test), len(wg_test[0])))\n",
    "\n",
    "print(\"tfidf and glove train shape: \", (len(tg_train), len(tg_train[0])))\n",
    "print(\"tfidf and glove dev shape: \", (len(tg_dev), len(tg_dev[0])))\n",
    "print(\"tfidf and glove test shape: \", (len(tg_test), len(tg_test[0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-pound",
   "metadata": {},
   "source": [
    "# To Save as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-broad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def savePrediction(fileName, prediction):\n",
    "    assert(len(test_id)==len(prediction))\n",
    "    with open(fileName, \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"tweet_id\", \"sentiment\"])\n",
    "        for prediction_index in range(len(prediction)):\n",
    "            row = [test_id[prediction_index], prediction[prediction_index]] \n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-algorithm",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from random import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-runner",
   "metadata": {},
   "source": [
    "<b> Baseline 1: Weighted Random Baseline </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-ireland",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_instances_train = len(y_train)\n",
    "count_instances_dev = len(y_dev)\n",
    "baseline_weight = Counter(y_train)\n",
    "\n",
    "pos_prob = baseline_weight[\"pos\"]/count_instances_train\n",
    "neu_prob = baseline_weight[\"neu\"]/count_instances_train\n",
    "neg_prob = baseline_weight[\"neg\"]/count_instances_train\n",
    "\n",
    "assert( pos_prob + neu_prob  + neg_prob  == 1)\n",
    "\n",
    "random_baseline = []\n",
    "\n",
    "for instance in range(count_instances_dev):\n",
    "    prob = random()\n",
    "    if prob < pos_prob:\n",
    "        random_baseline.append(\"pos\")\n",
    "    elif ((prob >= pos_prob) and (prob < (pos_prob + neu_prob))):\n",
    "        random_baseline.append(\"neu\")\n",
    "    elif (prob >= (pos_prob + neu_prob)):\n",
    "        random_baseline.append(\"neg\")\n",
    "\n",
    "random_baseline_acc = accuracy_score(random_baseline, y_dev)\n",
    "random_baseline_pre = precision_score(random_baseline, y_dev, average='macro')\n",
    "random_baseline_rec = recall_score(random_baseline, y_dev, average='macro')\n",
    "print(\"Random Baseline accuracy: \", random_baseline_acc )\n",
    "print(\"Random Baseline precision: \", random_baseline_pre )\n",
    "print(\"Random Baseline recall: \", random_baseline_rec )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-bridges",
   "metadata": {},
   "source": [
    "<b> Baseline 2: One-R Baseline </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_r = baseline_weight.most_common(1)[0][0]\n",
    "print(baseline_weight)\n",
    "print(one_r)\n",
    "\n",
    "one_r_baseline = [one_r for i in range(count_instances_dev)]\n",
    "\n",
    "one_r_baseline_acc = accuracy_score(one_r_baseline, y_dev)\n",
    "print(\"One-R Baseline accuracy: \", one_r_baseline_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-senator",
   "metadata": {},
   "source": [
    "# 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import eli5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-weekly",
   "metadata": {},
   "source": [
    "<b>1.1 Logistic Regression on Word Count only</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_lr = LogisticRegression(solver=\"saga\", max_iter=1000)\n",
    "\n",
    "w_lr.fit(w_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_lr_dev = w_lr.predict(w_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-helicopter",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_lr_dev_acc = accuracy_score(w_lr_dev, y_dev)\n",
    "w_lr_dev_pre = precision_score(w_lr_dev, y_dev, average=\"macro\")\n",
    "w_lr_dev_rec = recall_score(w_lr_dev, y_dev, average=\"macro\")\n",
    "print(\"Logistic Regression using Word Count accuracy: \", w_lr_dev_acc)\n",
    "print(\"Logistic Regression using Word Count precision: \", w_lr_dev_pre)\n",
    "print(\"Logistic Regression using Word Count recall: \", w_lr_dev_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-voice",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_lr_test = w_lr.predict(w_test)\n",
    "savePrediction(\"w_lr.csv\", w_lr_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-brunswick",
   "metadata": {},
   "source": [
    "<b>1.2 Logistic Regression on TF-IDF only</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_lr = LogisticRegression(solver=\"saga\", max_iter=1000)\n",
    "\n",
    "t_lr.fit(t_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-price",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_lr_dev = t_lr.predict(t_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-fraud",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_lr_dev_acc = accuracy_score(t_lr_dev, y_dev)\n",
    "t_lr_dev_pre = precision_score(t_lr_dev, y_dev, average=\"macro\")\n",
    "t_lr_dev_rec = recall_score(t_lr_dev, y_dev, average=\"macro\")\n",
    "print(\"Logistic Regression using TF-IDF accuracy: \", t_lr_dev_acc)\n",
    "print(\"Logistic Regression using TF-IDF precision: \", t_lr_dev_pre)\n",
    "print(\"Logistic Regression using TF-IDF recall: \", t_lr_dev_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-packaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(estimator=t_lr, \n",
    "                  feature_names= list(vocab_list),\n",
    "                 top=(50, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-circle",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_lr_test = t_lr.predict(t_test)\n",
    "savePrediction(\"t_lr.csv\", t_lr_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-terrace",
   "metadata": {},
   "source": [
    "<b> 1.3 Logistic Regression on Glove only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_lr = LogisticRegression(solver=\"saga\", max_iter=1000)\n",
    "\n",
    "g_lr.fit(g_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-prince",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_lr_dev = g_lr.predict(g_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_lr_dev_acc = accuracy_score(g_lr_dev, y_dev)\n",
    "g_lr_dev_pre = precision_score(g_lr_dev, y_dev, average=\"macro\")\n",
    "g_lr_dev_rec = recall_score(g_lr_dev, y_dev, average=\"macro\")\n",
    "print(\"Logistic Regression using Glove accuracy: \", g_lr_dev_acc )\n",
    "print(\"Logistic Regression using Glove precision: \", g_lr_dev_pre )\n",
    "print(\"Logistic Regression using Glove recall: \", g_lr_dev_rec )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_lr_test = g_lr.predict(g_test)\n",
    "savePrediction(\"g_lr.csv\", g_lr_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-calibration",
   "metadata": {},
   "source": [
    "<b> 1.4 Logistic Regression on Word Count and Glove </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-playlist",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_lr = LogisticRegression(solver=\"saga\", max_iter=1000)\n",
    "\n",
    "wg_lr.fit(wg_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-visibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_lr_dev = wg_lr.predict(wg_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-tonight",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_lr_dev_acc = accuracy_score(wg_lr_dev, y_dev)\n",
    "wg_lr_dev_pre = precision_score(wg_lr_dev, y_dev, average=\"macro\")\n",
    "wg_lr_dev_rec = recall_score(wg_lr_dev, y_dev, average=\"macro\")\n",
    "print(\"Logistic Regression using TF-IDF and Glove accuracy: \", wg_lr_dev_acc)\n",
    "print(\"Logistic Regression using TF-IDF and Glove precision: \", wg_lr_dev_pre)\n",
    "print(\"Logistic Regression using TF-IDF and Glove recall: \", wg_lr_dev_rec )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_lr_test = wg_lr.predict(wg_test)\n",
    "savePrediction(\"wg_lr.csv\", wg_lr_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-islam",
   "metadata": {},
   "source": [
    "<b> 1.5 Logistic Regression on TF-IDF and Glove </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-visiting",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_lr = LogisticRegression(solver=\"saga\", max_iter=1000)\n",
    "\n",
    "tg_lr.fit(tg_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_lr_dev = tg_lr.predict(tg_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-springfield",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_lr_dev_acc = accuracy_score(tg_lr_dev, y_dev)\n",
    "tg_lr_dev_pre = precision_score(tg_lr_dev, y_dev, average=\"macro\")\n",
    "tg_lr_dev_rec = recall_score(tg_lr_dev, y_dev, average=\"macro\")\n",
    "print(\"Logistic Regression using TF-IDF and Glove accuracy: \", tg_lr_dev_acc)\n",
    "print(\"Logistic Regression using TF-IDF and Glove precision: \", tg_lr_dev_pre)\n",
    "print(\"Logistic Regression using TF-IDF and Glove recall: \", tg_lr_dev_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_lr_test = tg_lr.predict(tg_test)\n",
    "savePrediction(\"tg_lr.csv\", tg_lr_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-variation",
   "metadata": {},
   "source": [
    "# 2. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-gilbert",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-fairy",
   "metadata": {},
   "source": [
    "<b> 2.1 SVM on Word Count only </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-broadcasting",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_svm = LinearSVC(max_iter=1000)\n",
    "\n",
    "w_svm.fit(w_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-tractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_svm_dev = w_svm.predict(w_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_svm_dev_acc = accuracy_score(w_svm_dev, y_dev)\n",
    "w_svm_dev_pre = precision_score(w_svm_dev, y_dev, average=\"macro\")\n",
    "w_svm_dev_rec = recall_score(w_svm_dev, y_dev, average=\"macro\")\n",
    "\n",
    "print(\"SVM using Word Count accuracy: \", w_svm_dev_acc )\n",
    "print(\"SVM using Word Count precision: \", w_svm_dev_pre )\n",
    "print(\"SVM using Word Count recall: \", w_svm_dev_rec )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_svm_test = w_svm.predict(w_test)\n",
    "savePrediction(\"w_svm.csv\", w_svm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-premium",
   "metadata": {},
   "source": [
    "<b> 2.2 SVM on TF-IDF only </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_svm = LinearSVC(max_iter=1000)\n",
    "\n",
    "t_svm.fit(t_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-closer",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_svm_dev = t_svm.predict(t_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_svm_dev_acc = accuracy_score(t_svm_dev, y_dev)\n",
    "t_svm_dev_pre = precision_score(t_svm_dev, y_dev, average=\"macro\")\n",
    "t_svm_dev_rec = recall_score(t_svm_dev, y_dev, average=\"macro\")\n",
    "print(\"SVM using TF-IDF accuracy: \", t_svm_dev_acc)\n",
    "print(\"SVM using TF-IDF precision: \", t_svm_dev_pre)\n",
    "print(\"SVM using TF-IDF recall: \", t_svm_dev_rec )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-benefit",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_svm_test = t_svm.predict(t_test)\n",
    "savePrediction(\"t_svm.csv\", t_svm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-malawi",
   "metadata": {},
   "source": [
    "<b> 2.3 SVM on Glove only </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_svm = LinearSVC(max_iter=1000)\n",
    "\n",
    "g_svm.fit(g_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_svm_dev = g_svm.predict(g_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_svm_dev_acc = accuracy_score(g_svm_dev, y_dev)\n",
    "g_svm_dev_pre = precision_score(g_svm_dev, y_dev, average=\"macro\")\n",
    "g_svm_dev_rec = recall_score(g_svm_dev, y_dev, average=\"macro\")\n",
    "print(\"SVM using Glove accuracy: \", g_svm_dev_acc)\n",
    "print(\"SVM using Glove precision: \", g_svm_dev_pre)\n",
    "print(\"SVM using Glove recall: \", g_svm_dev_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-airport",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_svm_test = g_svm.predict(g_test)\n",
    "savePrediction(\"g_svm.csv\", g_svm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-mexico",
   "metadata": {},
   "source": [
    "<b> 2.4 SVM on Word Count and Glove </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_svm = LinearSVC(max_iter=1000)\n",
    "\n",
    "wg_svm.fit(wg_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_svm_dev = wg_svm.predict(wg_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_svm_dev_acc = accuracy_score(wg_svm_dev, y_dev)\n",
    "wg_svm_dev_pre = precision_score(wg_svm_dev, y_dev, average=\"macro\")\n",
    "wg_svm_dev_rec = recall_score(wg_svm_dev, y_dev, average=\"macro\")\n",
    "print(\"SVM using Word Count and Glove accuracy: \", wg_svm_dev_acc)\n",
    "print(\"SVM using Word Count and Glove precisiony: \", wg_svm_dev_pre)\n",
    "print(\"SVM using Word Count and Glove recall: \", wg_svm_dev_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-lotus",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_svm_test = wg_svm.predict(wg_test)\n",
    "savePrediction(\"wg_svm.csv\", wg_svm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-morgan",
   "metadata": {},
   "source": [
    "<b> 2.5 SVM on TF-IDF and Glove </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_svm = LinearSVC(max_iter=1000)\n",
    "\n",
    "tg_svm.fit(tg_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_svm_dev = tg_svm.predict(tg_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_svm_dev_acc = accuracy_score(tg_svm_dev, y_dev)\n",
    "tg_svm_dev_pre = precision_score(tg_svm_dev, y_dev, average=\"macro\")\n",
    "tg_svm_dev_rec = recall_score(tg_svm_dev, y_dev, average=\"macro\")\n",
    "print(\"SVM using TF-IDF and Glove accuracy: \", tg_svm_dev_acc)\n",
    "print(\"SVM using TF-IDF and Glove precision: \", tg_svm_dev_pre)\n",
    "print(\"SVM using TF-IDF and Glove recall: \", tg_svm_dev_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-basis",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_svm_test = tg_svm.predict(tg_test)\n",
    "savePrediction(\"tg_svm.csv\", tg_svm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-plate",
   "metadata": {},
   "source": [
    "# 3. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-producer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-profit",
   "metadata": {},
   "source": [
    "<b> 3.1.a Multinomial Naive Bayes on Word Count only </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mnb = MultinomialNB()\n",
    "\n",
    "w_mnb.fit(w_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-machinery",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mnb_dev = w_mnb.predict(w_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mnb_dev_acc = accuracy_score(w_mnb_dev, y_dev)\n",
    "w_mnb_dev_pre = precision_score(w_mnb_dev, y_dev, average=\"macro\")\n",
    "w_mnb_dev_rec = recall_score(w_mnb_dev, y_dev, average=\"macro\")\n",
    "print(\"Multinomial Naive Bayes using Word Count accuracy: \", w_mnb_dev_acc)\n",
    "print(\"Multinomial Naive Bayes using Word Count precision: \", w_mnb_dev_pre)\n",
    "print(\"Multinomial Naive Bayes using Word Count recall: \", w_mnb_dev_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mnb_test = w_mnb.predict(w_test)\n",
    "savePrediction(\"w_mnb.csv\", w_mnb_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-absorption",
   "metadata": {},
   "source": [
    "<b> 3.1.b Gaussian Naive Bayes on Word Count only </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_gnb = GaussianNB()\n",
    "\n",
    "w_gnb.fit(w_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-legislature",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_gnb_dev = w_gnb.predict(w_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_gnb_dev_acc = accuracy_score(w_gnb_dev, y_dev)\n",
    "w_gnb_dev_pre = precision_score(w_gnb_dev, y_dev, average=\"macro\")\n",
    "w_gnb_dev_rec = recall_score(w_gnb_dev, y_dev, average=\"macro\")\n",
    "print(\"Gaussian Naive Bayes using Word Count accuracy: \", w_gnb_dev_acc)\n",
    "print(\"Gaussian Naive Bayes using Word Count precision: \", w_gnb_dev_pre)\n",
    "print(\"Gaussian Naive Bayes using Word Count recall: \", w_gnb_dev_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-montana",
   "metadata": {},
   "source": [
    "<b> 3.2.a Multinomial Naive Bayes on TF-IDF only </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_mnb = MultinomialNB()\n",
    "\n",
    "t_mnb.fit(t_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-gravity",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_mnb_dev = t_mnb.predict(t_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_mnb_dev_acc = accuracy_score(t_mnb_dev, y_dev)\n",
    "t_mnb_dev_pre = precision_score(t_mnb_dev, y_dev, average=\"macro\")\n",
    "t_mnb_dev_rec = recall_score(t_mnb_dev, y_dev, average=\"macro\")\n",
    "print(\"Multinomial Naive Bayes using TF-IDF accuracy: \", t_mnb_dev_acc)\n",
    "print(\"Multinomial Naive Bayes using TF-IDF precision: \", t_mnb_dev_pre)\n",
    "print(\"Multinomial Naive Bayes using TF-IDF recall: \", t_mnb_dev_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-timer",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_mnb_test = t_mnb.predict(t_test)\n",
    "savePrediction(\"t_mnb.csv\", t_mnb_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-announcement",
   "metadata": {},
   "source": [
    "<b> 3.2.b. Gaussian Naive Bayes on TF-IDF only </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_gnb = GaussianNB()\n",
    "\n",
    "t_gnb.fit(t_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_gnb_dev = t_gnb.predict(t_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_gnb_dev_acc = accuracy_score(t_gnb_dev, y_dev)\n",
    "t_gnb_dev_pre = precision_score(t_gnb_dev, y_dev, average=\"macro\")\n",
    "t_gnb_dev_rec = recall_score(t_gnb_dev, y_dev, average=\"macro\")\n",
    "print(\"Gaussian Naive Bayes using TF-IDF accuracy: \", t_gnb_dev_acc)\n",
    "print(\"Gaussian Naive Bayes using TF-IDF precision: \", t_gnb_dev_pre)\n",
    "print(\"Gaussian Naive Bayes using TF-IDF recall: \", t_gnb_dev_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_gnb_test = t_gnb.predict(t_test)\n",
    "savePrediction(\"t_gnb.csv\", t_gnb_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-cache",
   "metadata": {},
   "source": [
    "<b> 3.3 Gaussian Naive Bayes on Glove only </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-panama",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_gnb = GaussianNB()\n",
    "\n",
    "g_gnb.fit(g_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_gnb_dev = g_gnb.predict(g_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_gnb_dev_acc = accuracy_score(g_gnb_dev, y_dev)\n",
    "g_gnb_dev_pre = precision_score(g_gnb_dev, y_dev, average=\"macro\")\n",
    "g_gnb_dev_rec = recall_score(g_gnb_dev, y_dev, average=\"macro\")\n",
    "print(\"Gaussian Naive Bayes using Glove accuracy: \", g_gnb_dev_acc)\n",
    "print(\"Gaussian Naive Bayes using Glove precision: \", g_gnb_dev_pre)\n",
    "print(\"Gaussian Naive Bayes using Glove recall: \", g_gnb_dev_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_gnb_test = g_gnb.predict(g_test)\n",
    "savePrediction(\"g_gnb.csv\", g_gnb_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-links",
   "metadata": {},
   "source": [
    "<b> 3.4 Gaussian Naive Bayes on Word Count and Glove </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_gnb = GaussianNB()\n",
    "\n",
    "wg_gnb.fit(wg_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_gnb_dev = wg_gnb.predict(wg_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-employer",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_gnb_dev_acc = accuracy_score(wg_gnb_dev, y_dev)\n",
    "wg_gnb_dev_pre = precision_score(wg_gnb_dev, y_dev, average=\"macro\")\n",
    "wg_gnb_dev_rec = recall_score(wg_gnb_dev, y_dev, average=\"macro\")\n",
    "print(\"Gaussian Naive Bayes using Word Count and Glove accuracy: \", wg_gnb_dev_acc)\n",
    "print(\"Gaussian Naive Bayes using Word Count and Glove precision: \", wg_gnb_dev_pre)\n",
    "print(\"Gaussian Naive Bayes using Word Count and Glove recall: \", wg_gnb_dev_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_gnb_test = wg_gnb.predict(wg_test)\n",
    "savePrediction(\"wg_gnb.csv\", wg_gnb_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-bunny",
   "metadata": {},
   "source": [
    "<b> 3.5 Gaussian Naive Bayes on TF-IDF and Glove </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-philosophy",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_gnb = GaussianNB()\n",
    "\n",
    "tg_gnb.fit(tg_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_gnb_dev = tg_gnb.predict(tg_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_gnb_dev_acc = accuracy_score(tg_gnb_dev, y_dev)\n",
    "tg_gnb_dev_pre = precision_score(tg_gnb_dev, y_dev, average=\"macro\")\n",
    "tg_gnb_dev_rec = recall_score(tg_gnb_dev, y_dev, average=\"macro\")\n",
    "print(\"Gaussian Naive Bayes using TF-IDF and Glove accuracy: \", tg_gnb_dev_acc)\n",
    "print(\"Gaussian Naive Bayes using TF-IDF and Glove precision: \", tg_gnb_dev_pre)\n",
    "print(\"Gaussian Naive Bayes using TF-IDF and Glove recall: \", tg_gnb_dev_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_gnb_test = tg_gnb.predict(tg_test)\n",
    "savePrediction(\"tg_gnb.csv\", tg_gnb_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-print",
   "metadata": {},
   "source": [
    "# Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-vatican",
   "metadata": {},
   "source": [
    "Given the limitation in computing power, hyperparameter tuning will only be completed on three of the best performing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-holiday",
   "metadata": {},
   "source": [
    "<b> SVM </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC()\n",
    "\n",
    "param_grid = [    \n",
    "    {'C': [0.1, 1, 10],\n",
    "   'max_iter' : [500,1000,2500],\n",
    "   }\n",
    "]\n",
    "\n",
    "svm_search = GridSearchCV(svm, param_grid = param_grid, verbose=10, cv=3, error_score='raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-socket",
   "metadata": {},
   "source": [
    "<b> Tuned SVM on TF-IDF and Glove </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_svm_search = svm_search.fit(tg_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------ SVM on TF-IDF and Glove ------')\n",
    "print('Best Score: ', tg_svm_search.best_score_)\n",
    "print('Best Hyperparameters: ', tg_svm_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-webcam",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(tg_svm_search.cv_results_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-bonus",
   "metadata": {},
   "source": [
    "From above, the best score comes when C is 0.1; max_iteration = 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_svm_tuned = LinearSVC(C=0.1,max_iter = 500)\n",
    "tg_svm_tuned.fit(tg_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_svm_tuned_dev = tg_svm_tuned.predict(tg_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_svm_tuned_dev_acc = accuracy_score(tg_svm_tuned_dev, y_dev)\n",
    "tg_svm_tuned_dev_pre = precision_score(tg_svm_tuned_dev, y_dev, average=\"macro\")\n",
    "tg_svm_tuned_dev_rec = recall_score(tg_svm_tuned_dev, y_dev, average=\"macro\")\n",
    "print(\"Tuned SVM using TF-IDF and Glove accuracy: \", tg_svm_tuned_dev_acc)\n",
    "print(\"Tuned SVM using TF-IDF and Glove precision: \", tg_svm_tuned_dev_pre)\n",
    "print(\"Tuned SVM using TF-IDF and Glove recall: \", tg_svm_tuned_dev_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_svm_tuned_test = tg_svm_tuned.predict(tg_test)\n",
    "\n",
    "savePrediction(\"tg_svm_tuned.csv\",tg_svm_tuned_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-worth",
   "metadata": {},
   "source": [
    "<b> Tuned SVM on Word Count and Glove </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-humanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_svm_search = svm_search.fit(wg_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-spiritual",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------ SVM on Word Count and Glove ------')\n",
    "print('Best Score: ', wg_svm_search.best_score_)\n",
    "print('Best Hyperparameters: ', wg_svm_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(wg_svm_search.cv_results_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-vegetable",
   "metadata": {},
   "source": [
    "From above, the best score comes when C is 0.1; max_iteration = 500, 1000 and 2500 all produces the same score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_svm_tuned = LinearSVC(C=0.1,max_iter = 1000)\n",
    "wg_svm_tuned.fit(wg_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_svm_tuned_dev = wg_svm_tuned.predict(wg_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-eight",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_svm_tuned_dev_acc = accuracy_score(wg_svm_tuned_dev, y_dev)\n",
    "wg_svm_tuned_dev_pre = precision_score(wg_svm_tuned_dev, y_dev, average=\"macro\")\n",
    "wg_svm_tuned_dev_rec = recall_score(wg_svm_tuned_dev, y_dev, average=\"macro\")\n",
    "print(\"Tuned SVM using Word Count and Glove accuracy: \", wg_svm_tuned_dev_acc)\n",
    "print(\"Tuned SVM using Word Count and Glove precision: \", wg_svm_tuned_dev_pre)\n",
    "print(\"Tuned SVM using Word Count and Glove recall: \", wg_svm_tuned_dev_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_svm_tuned_test = wg_svm_tuned.predict(wg_test)\n",
    "\n",
    "savePrediction(\"wg_svm_tuned.csv\",wg_svm_tuned_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-trainer",
   "metadata": {},
   "source": [
    "<b> Logistic Regression </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-destruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver=\"saga\")\n",
    "\n",
    "param_grid = [    \n",
    "    {'C': [0.1, 1, 10],\n",
    "    'max_iter' : [500,1000,2500],\n",
    "    }\n",
    "]\n",
    "\n",
    "lr_search = GridSearchCV(lr, param_grid = param_grid, verbose=10, cv=3, error_score='raise')\n",
    "\n",
    "wg_lr_search = lr_search.fit(wg_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------ Logistic Regression on Word Count and Glove ------')\n",
    "print('Best Score: ', wg_lr_search.best_score_)\n",
    "print('Best Hyperparameters: ', wg_lr_search.best_params_)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(wg_lr_search.cv_results_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-pilot",
   "metadata": {},
   "source": [
    "From above, the best score comes when C is 0.1; max_iteration = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-karen",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_lr_tuned = LogisticRegression(solver=\"saga\",C=0.1,max_iter = 2500)\n",
    "wg_lr_tuned.fit(wg_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_lr_tuned_dev = wg_lr_tuned.predict(wg_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_lr_tuned_dev_acc = accuracy_score(wg_lr_tuned_dev, y_dev)\n",
    "wg_lr_tuned_dev_pre = precision_score(wg_lr_tuned_dev, y_dev, average=\"macro\")\n",
    "wg_lr_tuned_dev_rec = recall_score(wg_lr_tuned_dev, y_dev, average=\"macro\")\n",
    "print(\"Tuned Logistic Regression using Word Count and Glove accuracy: \", wg_lr_tuned_dev_acc)\n",
    "print(\"Tuned Logistic Regression using Word Count and Glove precision: \", wg_lr_tuned_dev_pre)\n",
    "print(\"Tuned Logistic Regression using Word Count and Glove recall: \", wg_lr_tuned_dev_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_lr_tuned_test = wg_lr_tuned.predict(wg_test)\n",
    "\n",
    "savePrediction(\"wg_lr_tuned.csv\",wg_lr_tuned_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-covering",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
